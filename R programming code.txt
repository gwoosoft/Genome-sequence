library(e1071)
library(caret)
library(caTools)
library(ROCR) 
require(nnet)
require(randomForest)
require(parallel)
library(doParallel) ## for parallel computing 
require(gbm)
require(ROCR)
require(xgboost)
require(Matrix)


cl <- makePSOCKcluster(5)
registerDoParallel(cl)

sdf<-read.csv('/home/2021/nyu/fall/gl1858/splice.csv')
my_cols <- c(names(sdf[2:61]))  ## take instance column names
mdf <-sdf[my_cols]
head(mdf)

output <- sdf[c(62)]
head(output)

onehotencoder <- function(df_orig) {
  df<-cbind(df_orig)
  df_clmtyp<-data.frame(clmtyp=sapply(df,class))
  df_col_typ<-data.frame(clmnm=colnames(df),clmtyp=df_clmtyp$clmtyp)
  for (rownm in 1:nrow(df_col_typ)) {
    if (df_col_typ[rownm,"clmtyp"]=="factor") {
      clmn_obj<-df[toString(df_col_typ[rownm,"clmnm"])] 
      dummy_matx<-data.frame(model.matrix( ~.-1, data = clmn_obj))
      dummy_matx<-dummy_matx[,c(1,3:ncol(dummy_matx))]
      df[toString(df_col_typ[rownm,"clmnm"])]<-NULL
      df<-cbind(df,dummy_matx)
      df[toString(df_col_typ[rownm,"clmnm"])]<-NULL
    }  }
  return(df)
}


cdf <- onehotencoder(mdf) 
data <-cbind(cdf,output)
head(data) 


set.seed(43)
randomized=data[sample(1:nrow(data),nrow(data)),] # Shuffle
tridx=sample(1:nrow(data),0.7*nrow(data),replace=F) #Get indices for 70% of the total number of samples
trdf=randomized[tridx,] # Define training data set
tstdf=randomized[-tridx,] # Define testing data set
table(data$Class)/nrow(data) # Check if class distribution is similar
table(trdf$Class)/nrow(trdf)
table(tstdf$Class)/nrow(tstdf)



trdf_RF=trdf  #Take train dataset 
trdf_RF$Class=as.factor(trdf_RF$Class)  #Take Y from the train dataset 

rf_model=randomForest(Class~.,trdf_RF, ntree=500) 
ry_pred = predict(rf_model, newdata = tstdf[-228])
cm = table(tstdf[, 228], ry_pred)
cfm<-confusionMatrix(cm)
cfm

plot(rf_model)


############################boosting 

gbm_model<-gbm(Class~., data=trdf, distribution="multinomial" ,n.trees=500,shrinkage=0.01,interaction.depth=3,
n.minobsinnode=10,
verbose=T,
keep.data=T)

gbm_predict<-predict(gbm_model, tstdf[,-c(228)], gbm_model$n.trees, type="response")
gbm_predicted<-round(gbm_predict)
labels = colnames(gbm_predicted)[apply(gbm_predicted, 1, which.max)]
#gbm_prediction<-prediction(labels,tstdf$Class)
p.gbm_predict=apply(gbm_predicted, 1, which.max)
result = data.frame(tstdf$Class, labels)
#print(result)
cm = confusionMatrix(tstdf$Class, as.factor(labels))
print(cm)

#gbm_perf<-performance(as.factor(labels), measure="tpr", x.measure="fpr")
#(gbm_auc<-performance(labels, measure="auc"))@y.values[[1]]
#plot(gbm_perf,main="ROC GBM n.tree=500")
#text(0.5,0.5,paste("AUC=" , format(gbm_auc@y.values[[1]],digits=5, scientific=FALSE)))

#par(pty="s")
#gbm_roc<-multiclass.roc(tstdf$Class~p.gbm_predict, plot=TRUE, print.auc=TRUE, col='blue', lwd=3, legacy.axes=TRUE, main='ROC Curves(GBM)')

##Xgboost

train_x = data.matrix(trdf[,-228])
train_y = trdf[,228]
 
test_x = data.matrix(tstdf[,-228])
test_y = tstdf[,228]


xgb_train = xgb.DMatrix(data=train_x, label=train_y)
xgb_test = xgb.DMatrix(data=test_x, label=test_y)


xgbc = xgboost(data=xgb_train, max.depth=3, nrounds=50)
print(xgbc)


pred = predict(xgbc, xgb_test)
#print(pred)
pred[(pred>3)] = 3
pred_y = as.factor((levels(test_y))[round(pred)])
cm = confusionMatrix(test_y, pred_y)
print(cm)

################### Stacking

stackeddf<-data.frame(actual=tstdf$Class,
rfpred=ry_pred,
gbmpred=as.factor(labels), ##gbm_pred
xgbpred=pred_y) ##xgb_pred

head(stackeddf)
tail(stackeddf)

data_new <- sapply(stackeddf[,1:4], unclass)    # Convert categorical variables
#data_new 

stacked_mean<-round(unlist(apply(data_new [,2 :4],1,mean))) #rounded so that it does not have to choose value between and it is more probabilistic. 
data_new<-cbind(data_new,stacked=stacked_mean)
testing<-as.data.frame(data_new) #to convert it back into 
(stbl<-table(testing$actual,testing$stacked))
(scfm<-caret::confusionMatrix(stbl))

stk_acc<-sum(diag(stbl))/sum(stbl)



#################### Compartive Analysis 

accuracy<-function(xt)sum(diag(xt))/sum(xt)

xgbtbl <-table(tstdf$Class,pred_y)

gbmtbl <-table(tstdf$Class,as.factor(labels))

rfmtbl <-table(tstdf$Class,ry_pred)

gbm_acc<-accuracy(gbmtbl)
rfm_acc<-accuracy(rfmtbl)
xgb_acc<-accuracy(xgbtbl)
stk_acc<-accuracy(stbl)




detach("package:caret", unload=TRUE) ##To avoid conflict with pRoc library 

require(pRoc)

gy_pred<-as.factor(labels)
mqa<-multiclass.roc(response=tstdf$Class, predictor=factor(gy_pred, ordered=TRUE),  plot=TRUE
,print.auc=TRUE, main='ROC Curves(GBM)')  ##GBM 
	




##RFM
rqa<-multiclass.roc(response=tstdf$Class, predictor=factor(ry_pred, ordered=TRUE), plot=TRUE
,print.auc=TRUE, main='ROC Curves(Rforest)')




##Xboost
xqa<-multiclass.roc(response=tstdf$Class, predictor=factor(pred_y, ordered=TRUE), plot=TRUE
,print.auc=TRUE, main='ROC Curves(Xboost)')




##Stacking
sqa<-multiclass.roc(response=tstdf$Class, predictor=factor(testing$stacked, ordered=TRUE), plot=TRUE,lwd=3,
legacy.axes=TRUE,print.auc=TRUE, main='ROC Curves(Stacked(RF,GBM,XGB))')


gbauc<-auc(mqa)
rtauc<-auc(rqa)
xbauc<-auc(xqa)
stauc<-auc(sqa)



perfdf<-data.frame(Algo=c("gbm","rfm","xgb","Stk"), Acc=c(gbm_acc, rfm_acc,xgb_acc,stk_acc), AUC=c(gbauc,rtauc,xbauc,stauc))
print(perfdf)


##Naive Bayes from Homework 2
NBA<-multiclass.roc(response=tstdf$Class, predictor=factor(nbpred, ordered=TRUE), plot=TRUE,lwd=3,
legacy.axes=TRUE,print.auc=TRUE, main='ROC Curves(Nayive Bayes)')



nbtbl <-table(tstdf$Class,nbpred)
nb_acc<-accuracy(nbtbl )
nbauc<-auc(NBA)



perfdf<-data.frame(Algo=c("gbm","rfm","xgb","Stk","NaiB"), Acc=c(gbm_acc, rfm_acc,xgb_acc,stk_acc,nb_acc)
, AUC=c(gbauc,rtauc,xbauc,stauc,nbauc))
print(perfdf)








